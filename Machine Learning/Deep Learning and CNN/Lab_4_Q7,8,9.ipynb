{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v498tQXdVqsT"
   },
   "source": [
    "### 7. Implement the convolution layer for 1 channel input and (nÂ¿=1) channel output. Implement both forward and backward passes. Implement the flatten operation\n",
    "### 8. (extra credit bonus:) generalize this for any number of input and any number of output channel. Implement both forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju-bPL0UWJGF"
   },
   "source": [
    "**Note:** We have directly implemented Q8 that is a Convolutional layer that can be used for any number of input and any number of output channels. Calling `Convolutional` layer with `outputChannelCnt == 1` and `inputShape[0] == 1` will solve the Q7. We have also implemented the flatten operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzjcsRG1BT31"
   },
   "outputs": [],
   "source": [
    "# Importing libraries and important modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import signal\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyyGhsJOBf4M"
   },
   "outputs": [],
   "source": [
    "# Class that represents the Convolutional Layer..\n",
    "class ConvolutionalLayer():\n",
    "\n",
    "    def __init__(self, inputShape, filterSize, outputChannelCnt):\n",
    "\n",
    "        # Size of the input image to be convoluted.\n",
    "        inputChannelCnt, inputHeight, inputWidth = inputShape\n",
    "\n",
    "        # The no of output channels we need\n",
    "        self.outputChannelCnt = outputChannelCnt\n",
    "\n",
    "        # Storing the values to class variables.\n",
    "        self.inputShape = inputShape\n",
    "        self.inputChannelCnt = inputChannelCnt\n",
    "\n",
    "        # Defining the shapes of output and kernels in terms of no of output channels , input shapes and kernel sizes.\n",
    "        self.outputShape = (outputChannelCnt, inputHeight - filterSize + 1, inputWidth - filterSize + 1)\n",
    "        self.kernelsShape = (outputChannelCnt, inputChannelCnt, filterSize, filterSize)\n",
    "\n",
    "        # Initiating the Kernels and biases randomly.\n",
    "        self.kernels = np.random.randn(*self.kernelsShape)\n",
    "        self.biases = np.random.randn(*self.outputShape)\n",
    "\n",
    "\n",
    "    def forwardPass(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "\n",
    "        # Iterating and filling the output by performing \"valid\" correlation operation using signal from scipy.\n",
    "        for i in range(self.outputChannelCnt):\n",
    "            for j in range(self.inputChannelCnt):\n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "   \n",
    "    def backwardPass(self, outputGrad, learningRate):\n",
    "        \n",
    "        # Initialising the partial differentiating of Kernel parameters and the input parameters.\n",
    "        kernelsGrad = np.zeros(self.kernelsShape)\n",
    "        inputGrad = np.zeros(self.inputShape)\n",
    "\n",
    "        # Using the scipy library to get del(L)/del(Kernel_i) or del(L)/del(b)\n",
    "        for i in range(self.outputChannelCnt):\n",
    "            for j in range(self.inputChannelCnt):\n",
    "                \n",
    "                # Iterating and filling:\n",
    "                #     The Kernels Gradient by performing \"valid\" correlation operation using signal from scipy.\n",
    "                #     The Input Gradient by performing \"full\" convolution operation using signal from scipy.\n",
    "\n",
    "                kernelsGrad[i, j] = signal.correlate2d(self.input[j], outputGrad[i], \"valid\")\n",
    "                inputGrad[j] += signal.convolve2d(outputGrad[i], self.kernels[i, j], \"full\")\n",
    "\n",
    "        # Updating the kernel weights & bias values using gradient descent.\n",
    "        self.kernels -= learningRate * kernelsGrad\n",
    "        self.biases -= learningRate * outputGrad\n",
    "        return inputGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzUrMEVmCQrt"
   },
   "outputs": [],
   "source": [
    "# Class that reshapes the given layer to the required dimensions.\n",
    "class FlattenLayer():\n",
    "    def __init__(self, inputShape, outputShape):\n",
    "        self.inputShape = inputShape\n",
    "        self.outputShape = outputShape\n",
    "\n",
    "    def forwardPass(self, input):\n",
    "        return np.reshape(input, self.outputShape)\n",
    "\n",
    "    def backwardPass(self, outputGrad, learningRate):\n",
    "        return np.reshape(outputGrad, self.inputShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_t6iZiUBf9S"
   },
   "outputs": [],
   "source": [
    "# Class that represents the usual Dense Layer of the Neural Network.\n",
    "class DenseLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "\n",
    "        # Initialising the weights & biases randomly. \n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forwardPass(self, input):\n",
    "        self.input = input\n",
    "\n",
    "        # N = WX + B\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backwardPass(self, outputGrad, learningRate):\n",
    "        wGrad = np.dot(outputGrad, self.input.T)\n",
    "        inputGrad = np.dot(self.weights.T, outputGrad)\n",
    "\n",
    "        # Updating the weights & biases using gradient descent.\n",
    "        self.weights -= learningRate * wGrad\n",
    "        self.bias -= learningRate * outputGrad\n",
    "        return inputGrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RvXdCDlBf69"
   },
   "outputs": [],
   "source": [
    "# Class that represents the activation functions of the Nueral Network.\n",
    "class ActivationLayer():\n",
    "    def __init__(self, activation, activationPrime):\n",
    "        self.activation = activation\n",
    "        self.activationPrime = activationPrime\n",
    "\n",
    "    def forwardPass(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backwardPass(self, outputGrad, learningRate):\n",
    "        return np.multiply(outputGrad, self.activationPrime(self.input))\n",
    "\n",
    "# Implementing the sigmoid activation function.\n",
    "class Sigmoid(ActivationLayer):\n",
    "    def __init__(self):\n",
    "\n",
    "        # For forward pass\n",
    "        def sigmoid(s):\n",
    "            return 1 / (1 + np.exp(-s))\n",
    "\n",
    "        # For backward pass\n",
    "        def sigmoidPrime(s):\n",
    "            s1 = sigmoid(s)\n",
    "            return s1 * (1 - s1)\n",
    "\n",
    "        super().__init__(sigmoid, sigmoidPrime)\n",
    "\n",
    "# Implementing the tanh activation function.\n",
    "class Tanh(ActivationLayer):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # For forward pass\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "         # For backward pass\n",
    "        def tanhPrime(x):\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "\n",
    "        super().__init__(tanh, tanhPrime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-3j7z1WV5mG"
   },
   "source": [
    "### 9. Train this CNN on mnist dataset. Layer 1: Convolution layer with 16 output channels+flatten+tanh activation. Layer 2: 10 output neuron with linear activation. Softmax cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNI6Ep4VCMU5"
   },
   "outputs": [],
   "source": [
    "# Y represents the True Y\n",
    "# P represents the Predicted Y.\n",
    "\n",
    "def crossEntropy(Y, P):\n",
    "    return np.mean(-Y * np.log(P) - (1 - Y) * np.log(1 - P))\n",
    "\n",
    "def crossEntropyPrime(Y, P):\n",
    "    return ((1 - Y) / (1 - P) - Y / P) / np.size(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j66CSIcmhl3F"
   },
   "outputs": [],
   "source": [
    "# Reading the MNIST Dataset & trimming the dataset so that training can be done faster.\n",
    "(xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "\n",
    "# Function that preprocesses the data & trims the dataset by taking \"cnt\" number of samples.\n",
    "def trimAndProcess(x, y, cnt):\n",
    "  \n",
    "    x = x[:cnt]\n",
    "    y = y[:cnt]\n",
    "\n",
    "    # Reshaping, normalising the x \n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "\n",
    "    # Encoding the y.\n",
    "    y = np_utils.to_categorical(y)\n",
    "    y = y.reshape(len(y), 10, 1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Now, trimming the training & testing sets using the above defined functions.\n",
    "xTrain, yTrain = trimAndProcess(xTrain, yTrain, 1000)\n",
    "xTest, yTest = trimAndProcess(xTest, yTest, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zw0st_XKCUPt"
   },
   "outputs": [],
   "source": [
    "# Function that just predicts the output of the network for the given input by doing forward passes across the layers. \n",
    "def predict(nNetwork, input):\n",
    "    output = input\n",
    "    for layer in nNetwork:\n",
    "        output = layer.forwardPass(output)\n",
    "    return output\n",
    "\n",
    "# The main function that takes the CNN and the training dataset & performs the forward & backward passes for given number of iterations. \n",
    "def train(nNetwork, xTrain, yTrain, iters, learningRate):\n",
    "    lossStore = []\n",
    "    for _ in range(iters):\n",
    "        loss = 0\n",
    "        for x, y in zip(xTrain, yTrain):\n",
    "\n",
    "            # Performing the forwardPass\n",
    "            output = predict(nNetwork, x)\n",
    "\n",
    "            # Accumulating the loss for each sample.\n",
    "            loss += crossEntropy(y, output)\n",
    "\n",
    "            # Performing the backwardPass\n",
    "            grad = crossEntropyPrime(y, output)\n",
    "            for layer in reversed(nNetwork):\n",
    "                grad = layer.backwardPass(grad, learningRate)\n",
    "\n",
    "        loss /= len(xTrain)\n",
    "        lossStore.append(loss)\n",
    "     \n",
    "    return lossStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrEyfyPqCW7l"
   },
   "outputs": [],
   "source": [
    "# Defining our neural network with all the necessary layers as per the requirements.\n",
    "nueralNetwork = [\n",
    "           \n",
    "    # 4 filters, 16 output channels\n",
    "    # (1 28 28) is the input channel, height & width\n",
    "    ConvolutionalLayer((1, 28, 28), 4, 16), \n",
    "\n",
    "    # Tanh Activation function.\n",
    "    Tanh(),\n",
    "    \n",
    "    # Flattening the input.\n",
    "    FlattenLayer((16, 25, 25), (16 * 25 * 25, 1)),\n",
    "\n",
    "    # Dense layer with 100 output neurons.\n",
    "    DenseLayer(16 * 25 * 25, 100),\n",
    "    Sigmoid(),\n",
    "\n",
    "    # Final layer with 10 output nuerons.\n",
    "    DenseLayer(100, 10),\n",
    "    Sigmoid()\n",
    "]\n",
    "\n",
    "# Training the CNN using the function defined earlier for 100 iters with a learning rate of 0.2\n",
    "lossStore = train( nueralNetwork, xTrain, yTrain, 100, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "sBtIXZwXodns",
    "outputId": "de2484be-2638-4dc2-91cd-70eb7791bbc7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(lossStore)),lossStore)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss on Training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kHxuGhlo_3o"
   },
   "source": [
    "#### Checking the CNN performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCywIPBroYaU",
    "outputId": "8adcc844-7155-40ec-e39a-d7d9c078a536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of the samples classified correctly is:  76.2 %\n"
     ]
    }
   ],
   "source": [
    "correct_cnt = 0\n",
    "\n",
    "for x, y in zip(xTest, yTest):\n",
    "    output = predict(nueralNetwork, x)\n",
    "    \n",
    "    if (np.argmax(output) == np.argmax(y)):\n",
    "       correct_cnt += 1\n",
    "\n",
    "print(\"The percentage of the samples classified correctly is: \", (correct_cnt/len(xTest))*100,\"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LE_O82VX45g"
   },
   "source": [
    "### Observations:\n",
    " - We can observe that the loss on the training dataset is decreasing overall with increase in the number of iterations as per the graph plotted above. \n",
    " - Due to the presence of numerous paramters that are being learnt in the CNN, even for training for around 1000 iterations, it is consuming a lot of time, So, we trained for just 100 iterations.  The percentage of the samples that are classified correctly by the CNN is around `76 - 85%` when trained for 100 iterations.\n",
    " - It is observable that training for even more iterations & better tuning the hyper parameter, learning rate will give an even better performance of our CNN.  \n",
    "\n",
    "### References:\n",
    "- https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1\n",
    "- https://towardsdatascience.com/a-guide-to-convolutional-neural-networks-from-scratch-f1e3bfc3e2de\n",
    "- https://towardsdatascience.com/covolutional-neural-network-cb0883dd6529"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_4_Q7,8,9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
